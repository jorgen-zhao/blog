---
title: Flinkä¸å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿé›†æˆ-æ„å»ºç»Ÿä¸€æ•°æ®å¤„ç†å¹³å°çš„å…³é”®
date: 2026-02-04
tags: [Flink, å¤§æ•°æ®ç”Ÿæ€, æ•°æ®é›†æˆ]
---

## å‰è¨€

å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯Jorgenï¼ğŸ‘‹ åœ¨ä¹‹å‰çš„åšå®¢ä¸­ï¼Œæˆ‘ä»¬å·²ç»æ·±å…¥æ¢è®¨äº†Flinkçš„å„ä¸ªæ–¹é¢ï¼Œä»æ¶æ„åŸç†åˆ°æ€§èƒ½ä¼˜åŒ–ï¼Œä»çŠ¶æ€ç®¡ç†åˆ°æ—¶é—´è¯­ä¹‰ã€‚ç„¶è€Œï¼Œåœ¨å®é™…çš„å¤§æ•°æ®ç¯å¢ƒä¸­ï¼ŒFlinkå¾ˆå°‘å•ç‹¬å·¥ä½œï¼Œå®ƒé€šå¸¸éœ€è¦ä¸å…¶ä»–å¤§æ•°æ®ç»„ä»¶ååŒå·¥ä½œï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„æ•°æ®å¤„ç†å¹³å°ã€‚

ä»Šå¤©ï¼Œæˆ‘æƒ³å’Œå¤§å®¶èŠèŠFlinkå¦‚ä½•ä¸å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿä¸­çš„å…¶ä»–ç»„ä»¶é›†æˆï¼ŒåŒ…æ‹¬Hadoopã€Sparkã€Hiveã€Kafkaç­‰ã€‚è¿™äº›é›†æˆä¸ä»…èƒ½å‘æŒ¥å„ç»„ä»¶çš„ä¼˜åŠ¿ï¼Œè¿˜èƒ½è§£å†³å®é™…ä¸šåŠ¡åœºæ™¯ä¸­çš„å¤æ‚æ•°æ®å¤„ç†éœ€æ±‚ã€‚

## å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿæ¦‚è§ˆ

åœ¨æ·±å…¥æ¢è®¨Flinkä¸å„ç»„ä»¶çš„é›†æˆä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆå¿«é€Ÿå›é¡¾ä¸€ä¸‹ç°ä»£å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿçš„ä¸»è¦ç»„ä»¶ï¼š

| ç»„ä»¶ç±»åˆ« | ä»£è¡¨ç»„ä»¶ | ä¸»è¦åŠŸèƒ½ |
|---------|---------|---------|
| æ•°æ®å­˜å‚¨ | HDFSã€HBaseã€Kuduã€S3 | åˆ†å¸ƒå¼å­˜å‚¨ç»“æ„åŒ–/éç»“æ„åŒ–æ•°æ® |
| æ•°æ®å¤„ç† | MapReduceã€Sparkã€Flink | æ‰¹å¤„ç†ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹  |
| æ•°æ®ä»“åº“ | Hiveã€Impalaã€Presto | SQLæŸ¥è¯¢ã€æ•°æ®ä»“åº“ç®¡ç† |
| æ¶ˆæ¯é˜Ÿåˆ— | Kafkaã€Pulsarã€RabbitMQ | æ¶ˆæ¯ä¼ é€’ã€äº‹ä»¶æµå¤„ç† |
| èµ„æºç®¡ç† | YARNã€Kubernetes | èµ„æºè°ƒåº¦ã€å®¹å™¨ç®¡ç† |
| å·¥ä½œæµè°ƒåº¦ | Airflowã€Oozieã€Azkaban | ä»»åŠ¡ç¼–æ’ã€ä¾èµ–ç®¡ç† |

::: tip
ç†è§£å„ç»„ä»¶çš„ç‰¹æ€§å’Œä¼˜åŠ¿ï¼Œæ˜¯æ„å»ºé«˜æ•ˆå¤§æ•°æ®å¹³å°çš„ç¬¬ä¸€æ­¥ã€‚Flinkä½œä¸ºæµå¤„ç†å¼•æ“ï¼Œåœ¨å®æ—¶æ•°æ®å¤„ç†æ–¹é¢å…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œä½†å®ƒéœ€è¦ä¸å…¶ä»–ç»„ä»¶é…åˆæ‰èƒ½å‘æŒ¥æœ€å¤§ä»·å€¼ã€‚
:::

## Flinkä¸Hadoopç”Ÿæ€çš„é›†æˆ

### Flinkä¸HDFSé›†æˆ

HDFSï¼ˆHadoop Distributed File Systemï¼‰æ˜¯Hadoopç”Ÿæ€ç³»ç»Ÿçš„æ ¸å¿ƒå­˜å‚¨ç»„ä»¶ã€‚Flinkå¯ä»¥ç›´æ¥è¯»å†™HDFSä¸Šçš„æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼Œå¦‚CSVã€JSONã€Parquetã€Avroç­‰ã€‚

```java
// ä»HDFSè¯»å–CSVæ–‡ä»¶
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
DataSet<String> csvInput = env.readTextFile("hdfs://namenode:8020/path/to/data.csv");

// å°†ç»“æœå†™å…¥HDFS
DataSet<Tuple2<String, Integer>> result = ...;
result.writeAsCsv("hdfs://namenode:8020/output/result.csv");
```

ğŸ¤” **å°è´´å£«**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå»ºè®®ä½¿ç”¨Flinkçš„`hadoop`åŒ…æ¥ç¡®ä¿ä¸HDFSç‰ˆæœ¬çš„å…¼å®¹æ€§ã€‚

### Flinkä¸Hiveé›†æˆ

Hiveä½œä¸ºæ•°æ®ä»“åº“å·¥å…·ï¼Œæä¾›äº†SQLæ¥å£æ¥æŸ¥è¯¢å­˜å‚¨åœ¨HDFSä¸Šçš„æ•°æ®ã€‚Flinkå¯ä»¥é€šè¿‡HiveCatalogä¸Hiveé›†æˆï¼Œå®ç°ï¼š

1. è¯»å†™Hiveè¡¨
2. æ‰§è¡ŒHiveQLæŸ¥è¯¢
3. å¤ç”¨Hiveçš„å…ƒæ•°æ®

```java
// åˆ›å»ºHiveCatalog
HiveCatalog hiveCatalog = new HiveCatalog("my_catalog", "my_database", "/path/to/hive/conf", "3.1.2");

// æ³¨å†ŒHiveCatalog
tableEnv.registerCatalog("my_catalog", hiveCatalog);

// ä½¿ç”¨Hiveè¡¨
Table hiveTable = tableEnv.sqlQuery("SELECT * FROM my_catalog.my_database.my_table");
```

### Flinkä¸HBaseé›†æˆ

HBaseæ˜¯æ„å»ºåœ¨HDFSä¹‹ä¸Šçš„NoSQLæ•°æ®åº“ï¼Œé€‚åˆå­˜å‚¨å¤§è§„æ¨¡ç»“æ„åŒ–æ•°æ®ã€‚Flinkæä¾›äº†ä¸“é—¨çš„è¿æ¥å™¨æ¥ä¸HBaseäº¤äº’ï¼š

```java
// é…ç½®HBaseè¿æ¥å‚æ•°
Configuration config = HBaseConfiguration.create();
config.set("hbase.zookeeper.quorum", "zk1:2181,zk2:2181,zk3:2181");

// åˆ›å»ºHBaseè¿æ¥
TableEnvironment tableEnv = ...;
tableEnv.executeSql(
  "CREATE TABLE hbase_table (" +
  "rowkey STRING," +
  "cf ROW<col1 STRING, col2 INT>," +
  "PRIMARY KEY (rowkey) NOT ENFORCED" +
  ") WITH (" +
  " 'connector' = 'hbase-2.2'," +
  " 'table-name' = 'my_table'," +
  " 'zookeeper.quorum' = 'zk1:2181,zk2:2181,zk3:2181'" +
  ")"
);
```

## Flinkä¸Sparkç”Ÿæ€çš„ååŒ

è™½ç„¶Sparkå’ŒFlinkéƒ½æ˜¯å¤§æ•°æ®å¤„ç†æ¡†æ¶ï¼Œä½†å®ƒä»¬å„è‡ªæœ‰ä¸åŒçš„ä¼˜åŠ¿å’Œé€‚ç”¨åœºæ™¯ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå®ƒä»¬å¸¸å¸¸ååŒå·¥ä½œï¼š

### Flinkä¸Spark Streamingå¯¹æ¯”

| ç‰¹æ€§ | Flink | Spark Streaming |
|------|-------|----------------|
| å¤„ç†æ¨¡å‹ | çœŸæ­£çš„æµå¤„ç† | å¾®æ‰¹å¤„ç† |
| å»¶è¿Ÿ | æ¯«ç§’çº§ | ç§’çº§ |
| çŠ¶æ€ç®¡ç† | åŸç”Ÿæ”¯æŒ | éœ€è¦é¢å¤–é…ç½® |
| å®¹é”™æœºåˆ¶ | Checkpointing | RDD Lineage |
| æ—¶é—´è¯­ä¹‰ | æ”¯æŒäº‹ä»¶æ—¶é—´ | ä¸»è¦å¤„ç†å¤„ç†æ—¶é—´ |

### Flinkä¸Spark MLlibçš„é›†æˆ

è™½ç„¶Flinkæœ‰è‡ªå·±çš„æœºå™¨å­¦ä¹ åº“FlinkMLï¼Œä½†åœ¨æŸäº›åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦åˆ©ç”¨Spark MLlibçš„ç®—æ³•ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°é›†æˆï¼š

1. ä½¿ç”¨Kafkaä½œä¸ºä¸­é—´æ¶ˆæ¯é˜Ÿåˆ—ï¼Œå°†æ•°æ®ä»Flinkå‘é€åˆ°Spark
2. ä½¿ç”¨JDBCè¿æ¥ï¼Œå°†Flinkå¤„ç†åçš„æ•°æ®å†™å…¥å…³ç³»å‹æ•°æ®åº“ï¼Œä¾›Sparkè¯»å–

## Flinkä¸æ¶ˆæ¯é˜Ÿåˆ—çš„é›†æˆ

æ¶ˆæ¯é˜Ÿåˆ—æ˜¯ç°ä»£æ•°æ®æ¶æ„ä¸­çš„æ ¸å¿ƒç»„ä»¶ï¼ŒFlinkä¸ä¸»æµæ¶ˆæ¯é˜Ÿåˆ—çš„é›†æˆæ˜¯å…¶æµå¤„ç†èƒ½åŠ›çš„å…³é”®ã€‚

### Flinkä¸Kafkaé›†æˆ

Kafkaæ˜¯æœ€å¸¸ç”¨çš„æ¶ˆæ¯é˜Ÿåˆ—ä¹‹ä¸€ï¼ŒFlinkæä¾›äº†ä¸“é—¨çš„é«˜æ•ˆè¿æ¥å™¨ï¼š

```java
// åˆ›å»ºKafkaæ¶ˆè´¹è€…
Properties properties = new Properties();
properties.setProperty("bootstrap.servers", "kafka-broker:9092");
properties.setProperty("group.id", "flink-consumer");

FlinkKafkaConsumer<String> kafkaSource = new FlinkKafkaConsumer<>(
  "input-topic",
  new SimpleStringSchema(),
  properties
);

// æ·»åŠ åˆ°Flinkä½œä¸š
DataStream<String> stream = env.addSource(kafkaSource);

// å¤„ç†æ•°æ®å¹¶å†™å…¥Kafka
DataStream<String> resultStream = stream.map(...);
FlinkKafkaProducer<String> kafkaSink = new FlinkKafkaProducer<>(
  "output-topic",
  new KeyedSerializationSchemaWrapper<>(new SimpleStringSchema()),
  properties,
  FlinkKafkaProducer.Semantic.EXACTLY_ONCE
);
resultStream.addSink(kafkaSink);
```

### Flinkä¸Pulsaré›†æˆ

Pulsaræ˜¯å¦ä¸€ç§æµè¡Œçš„åˆ†å¸ƒå¼æ¶ˆæ¯ç³»ç»Ÿï¼ŒFlinkä¹Ÿæä¾›äº†Pulsarè¿æ¥å™¨ï¼š

```java
// åˆ›å»ºPulsaræ¶ˆè´¹è€…
PulsarSource<String> pulsarSource = PulsarSource.builder()
  .serviceUrl("pulsar://localhost:6650")
  .topics("persistent://my-tenant/my-namespace/my-topic")
  .serializationSchema(new SimpleStringSchema())
  .build();

// æ·»åŠ åˆ°Flinkä½œä¸š
DataStream<String> stream = env.addSource(pulsarSource);

// å¤„ç†æ•°æ®å¹¶å†™å…¥Pulsar
DataStream<String> resultStream = stream.map(...);
PulsarSink<String> pulsarSink = PulsarSink.builder()
  .serviceUrl("pulsar://localhost:6650")
  .topic("persistent://my-tenant/my-namespace/output-topic")
  .serializationSchema(new SimpleStringSchema())
  .build();
resultStream.addSink(pulsarSink);
```

## Flinkä¸äº‘åŸç”ŸæŠ€æœ¯çš„é›†æˆ

éšç€äº‘åŸç”ŸæŠ€æœ¯çš„å‘å±•ï¼ŒFlinkä¹Ÿåœ¨ä¸æ–­é€‚åº”è¿™ä¸€è¶‹åŠ¿ï¼Œä¸Kubernetesç­‰äº‘åŸç”ŸæŠ€æœ¯æ·±åº¦é›†æˆã€‚

### Flinkä¸Kubernetesé›†æˆ

Flink 1.12åŠä»¥ä¸Šç‰ˆæœ¬åŸç”Ÿæ”¯æŒåœ¨Kubernetesä¸Šéƒ¨ç½²ï¼Œæä¾›äº†ä»¥ä¸‹ä¼˜åŠ¿ï¼š

1. è‡ªåŠ¨æ‰©å±•ï¼šæ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´Flinkä½œä¸šèµ„æº
2. è‡ªæ„ˆèƒ½åŠ›ï¼šèŠ‚ç‚¹æ•…éšœæ—¶è‡ªåŠ¨é‡æ–°è°ƒåº¦
3. èµ„æºéš”ç¦»ï¼šé€šè¿‡å®¹å™¨å®ç°èµ„æºéš”ç¦»
4. ç®€åŒ–è¿ç»´ï¼šä½¿ç”¨Kubernetes APIç®¡ç†Flinkä½œä¸š

```yaml
# Flinkä½œä¸šçš„Kuberneteséƒ¨ç½²ç¤ºä¾‹
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flink-job
spec:
  replicas: 1
  selector:
    matchLabels:
      app: flink-job
  template:
    metadata:
      labels:
        app: flink-job
    spec:
      containers:
      - name: flink
        image: flink:1.14.0-scala_2.12-java8
        command: ["jobmanager"]
        ports:
        - containerPort: 8081
        env:
        - name: FLINK_PROPERTIES
          value: "jobmanager.rpc.address: flink-job"
```

### Flinkä¸Serverlessæ¶æ„

Serverlessæ¶æ„æ˜¯äº‘åŸç”Ÿé¢†åŸŸçš„å¦ä¸€è¶‹åŠ¿ï¼ŒFlinkå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸Serverlessæ¶æ„é›†æˆï¼š

1. ä½¿ç”¨Flink on Kuberneteså®ç°Serverlessæµå¤„ç†
2. ç»“åˆAWS Lambdaã€Azure Functionsç­‰å®ç°äº‹ä»¶é©±åŠ¨çš„Serverlessæ¶æ„
3. ä½¿ç”¨Flinkå¤„ç†æµæ•°æ®ï¼Œç„¶åè§¦å‘Serverlesså‡½æ•°è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†

## å®é™…åº”ç”¨æ¡ˆä¾‹ï¼šæ„å»ºå®æ—¶æ•°æ®ç®¡é“

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®é™…æ¡ˆä¾‹ï¼Œçœ‹çœ‹å¦‚ä½•å°†Flinkä¸å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿä¸­çš„å…¶ä»–ç»„ä»¶ç»“åˆï¼Œæ„å»ºä¸€ä¸ªå®æ—¶æ•°æ®ç®¡é“ã€‚

### åœºæ™¯æè¿°

å‡è®¾æˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªå®æ—¶æ•°æ®å¤„ç†ç®¡é“ï¼Œå¤„ç†æ¥è‡ªIoTè®¾å¤‡çš„æ•°æ®ï¼Œè¿›è¡Œå®æ—¶åˆ†æå’Œå­˜å‚¨ï¼Œå¹¶æ”¯æŒå®æ—¶æŸ¥è¯¢å’Œå¯è§†åŒ–ã€‚

### æ¶æ„è®¾è®¡

```
IoTè®¾å¤‡ â†’ Kafka â†’ Flink â†’ HDFS/HBase â†’ Hive/Impala â†’ BIå·¥å…·
                â†“
            Elasticsearch â†’ Kibana
```

### å®ç°æ­¥éª¤

1. **æ•°æ®é‡‡é›†**ï¼šIoTè®¾å¤‡å°†æ•°æ®å‘é€åˆ°Kafkaä¸»é¢˜
2. **å®æ—¶å¤„ç†**ï¼šFlinkæ¶ˆè´¹Kafkaä¸­çš„æ•°æ®ï¼Œè¿›è¡Œå®æ—¶å¤„ç†å’Œèšåˆ
3. **æ•°æ®å­˜å‚¨**ï¼šå°†å¤„ç†åçš„ç»“æœå­˜å‚¨åˆ°HDFSå’ŒHBase
4. **æ•°æ®ä»“åº“**ï¼šå°†æ•°æ®åŠ è½½åˆ°Hive/Impalaï¼Œæ”¯æŒSQLæŸ¥è¯¢
5. **å®æ—¶åˆ†æ**ï¼šå°†å¤„ç†åçš„æ•°æ®ç´¢å¼•åˆ°Elasticsearchï¼Œæ”¯æŒå®æ—¶æœç´¢
6. **æ•°æ®å¯è§†åŒ–**ï¼šä½¿ç”¨Kibanaç­‰å·¥å…·è¿›è¡Œæ•°æ®å¯è§†åŒ–

### å…³é”®ä»£ç ç‰‡æ®µ

```java
// 1. ä»Kafkaæ¶ˆè´¹æ•°æ®
Properties kafkaProps = new Properties();
kafkaProps.setProperty("bootstrap.servers", "kafka-broker:9092");
kafkaProps.setProperty("group.id", "iot-data-processor");

FlinkKafkaConsumer<SensorReading> source = new FlinkKafkaConsumer<>(
  "iot-sensor-data",
  new SensorReadingDeserializer(),
  kafkaProps
);

DataStream<SensorReading> sensorStream = env.addSource(source);

// 2. å®æ—¶å¤„ç†æ•°æ®
DataStream<SensorAggregation> aggregatedStream = sensorStream
  .keyBy("sensorId")
  .timeWindow(Time.seconds(10))
  .aggregate(new SensorAggregator());

// 3. å†™å…¥HBase
aggregatedStream.addSink(new HBaseSink("sensor-aggregations"));

// 4. å†™å…¥HDFS
aggregatedStream.writeAsCsv("hdfs://namenode:8020/iot/aggregated");

// 5. å†™å…¥Elasticsearch
aggregatedStream.addSink(new ElasticsearchSink.Builder<>(
  "http://elasticsearch:9200",
  new ElasticsearchSinkFunction<SensorAggregation>() {
    // å®ç°å†™å…¥é€»è¾‘
  }
).build());
```

## æœ€ä½³å®è·µä¸æ³¨æ„äº‹é¡¹

åœ¨å°†Flinkä¸å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿé›†æˆæ—¶ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›æœ€ä½³å®è·µå’Œæ³¨æ„äº‹é¡¹ï¼š

### 1. æ•°æ®æ ¼å¼ä¸€è‡´æ€§

ç¡®ä¿åœ¨ä¸åŒç³»ç»Ÿä¹‹é—´ä¼ è¾“çš„æ•°æ®æ ¼å¼ä¸€è‡´ã€‚æ¨èä½¿ç”¨JSONã€Avroæˆ–Parquetç­‰é€šç”¨æ ¼å¼ã€‚

### 2. æ€§èƒ½ä¼˜åŒ–

- åˆç†è®¾ç½®å¹¶è¡Œåº¦
- ä½¿ç”¨çŠ¶æ€åç«¯ï¼ˆå¦‚RocksDBï¼‰ç®¡ç†å¤§çŠ¶æ€
- ä¼˜åŒ–çª—å£å¤§å°å’Œè§¦å‘æ¡ä»¶
- ä½¿ç”¨å¼‚æ­¥I/Oæé«˜ååé‡

### 3. å®¹é”™ä¸å¯é æ€§

- å¯ç”¨Checkpointingæœºåˆ¶
- é…ç½®é€‚å½“çš„é‡å¯ç­–ç•¥
- ç¡®ä¿Kafkaç­‰å¤–éƒ¨ç³»ç»Ÿçš„å¯é æ€§

### 4. ç›‘æ§ä¸å¯è§‚æµ‹æ€§

- é›†æˆPrometheuså’ŒGrafanaè¿›è¡Œç›‘æ§
- ä½¿ç”¨Flinkçš„REST APIè·å–ä½œä¸šçŠ¶æ€
- å®ç°è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†

### 5. å®‰å…¨è€ƒè™‘

- å¯ç”¨SSL/TLSåŠ å¯†
- é…ç½®é€‚å½“çš„è®¤è¯å’Œæˆæƒ
- æ•æ„Ÿæ•°æ®åŠ å¯†å­˜å‚¨

## æœªæ¥å±•æœ›

éšç€å¤§æ•°æ®å’Œå®æ—¶æ•°æ®å¤„ç†éœ€æ±‚çš„ä¸æ–­å¢é•¿ï¼ŒFlinkä¸å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿçš„é›†æˆå°†å˜å¾—æ›´åŠ ç´§å¯†å’Œæ™ºèƒ½ã€‚æœªæ¥ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…ä»¥ä¸‹å‘å±•è¶‹åŠ¿ï¼š

1. **æ›´ç´§å¯†çš„äº‘åŸç”Ÿé›†æˆ**ï¼šFlinkå°†ä¸Kubernetesç­‰äº‘åŸç”Ÿå¹³å°æ›´æ·±åº¦é›†æˆï¼Œæä¾›æ›´è‡ªåŠ¨åŒ–çš„éƒ¨ç½²å’Œè¿ç»´èƒ½åŠ›ã€‚
2. **è¾¹ç¼˜è®¡ç®—ä¸æµå¤„ç†çš„èåˆ**ï¼šFlinkå°†æ›´å¤šåœ°æ”¯æŒè¾¹ç¼˜è®¡ç®—åœºæ™¯ï¼Œå®ç°ç«¯åˆ°ç«¯çš„æµå¤„ç†ç®¡é“ã€‚
3. **AI/MLä¸æµå¤„ç†çš„ç»“åˆ**ï¼šFlinkå°†æ›´å¥½åœ°æ”¯æŒå®æ—¶æœºå™¨å­¦ä¹ å’ŒAIæ¨ç†ï¼Œæ„å»ºæ™ºèƒ½æµå¤„ç†åº”ç”¨ã€‚
4. **Serverlessæµå¤„ç†**ï¼šFlinkå°†æ›´å¥½åœ°æ”¯æŒServerlessæ¶æ„ï¼Œå®ç°æŒ‰éœ€ä»˜è´¹çš„æµå¤„ç†æœåŠ¡ã€‚
5. **è·¨äº‘å’Œæ··åˆäº‘æ”¯æŒ**ï¼šFlinkå°†æ›´å¥½åœ°æ”¯æŒè·¨äº‘å¹³å°å’Œæ··åˆäº‘éƒ¨ç½²ï¼Œæä¾›æ›´çµæ´»çš„æ•°æ®å¤„ç†èƒ½åŠ›ã€‚

## ç»“è¯­

é€šè¿‡ä»Šå¤©çš„åˆ†äº«ï¼Œæˆ‘ä»¬ä¸€èµ·æ¢è®¨äº†Flinkå¦‚ä½•ä¸å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿä¸­çš„å…¶ä»–ç»„ä»¶é›†æˆï¼Œæ„å»ºç»Ÿä¸€çš„æ•°æ®å¤„ç†å¹³å°ã€‚ä»Hadoopç”Ÿæ€åˆ°Sparkç”Ÿæ€ï¼Œä»æ¶ˆæ¯é˜Ÿåˆ—åˆ°äº‘åŸç”ŸæŠ€æœ¯ï¼ŒFlinkå‡­å€Ÿå…¶å¼ºå¤§çš„æµå¤„ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸å„ç§ç»„ä»¶æ— ç¼é›†æˆï¼Œæ»¡è¶³å¤šæ ·åŒ–çš„æ•°æ®å¤„ç†éœ€æ±‚ã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®å…·ä½“ä¸šåŠ¡åœºæ™¯å’Œéœ€æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„ç»„ä»¶ç»„åˆï¼Œå¹¶éµå¾ªæœ€ä½³å®è·µï¼Œæ„å»ºé«˜æ•ˆã€å¯é ã€å¯æ‰©å±•çš„æ•°æ®å¤„ç†å¹³å°ã€‚

å¸Œæœ›ä»Šå¤©çš„åˆ†äº«å¯¹å¤§å®¶æœ‰æ‰€å¸®åŠ©ï¼å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨è¯„è®ºåŒºç•™è¨€äº¤æµã€‚ğŸ˜Š

> "åœ¨å¤§æ•°æ®çš„ä¸–ç•Œé‡Œï¼Œæ²¡æœ‰æœ€å¥½çš„æŠ€æœ¯ï¼Œåªæœ‰æœ€é€‚åˆçš„æŠ€æœ¯ã€‚é€‰æ‹©åˆé€‚çš„ç»„ä»¶ç»„åˆï¼Œæ‰èƒ½æ„å»ºçœŸæ­£æœ‰ä»·å€¼çš„è§£å†³æ–¹æ¡ˆã€‚"