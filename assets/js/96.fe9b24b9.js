(window.webpackJsonp=window.webpackJsonp||[]).push([[96],{448:function(t,s,a){"use strict";a.r(s);var n=a(15),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"前言"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#前言"}},[t._v("#")]),t._v(" 前言")]),t._v(" "),s("p",[t._v("在大型语言模型(LLM)的世界里，Transformer架构无疑是革命性的存在。自从2017年Google的论文《Attention Is All You Need》提出这一架构以来，它几乎成为了所有现代大语言模型的基础。然而，很多初学者在面对Transformer时，常常被其复杂的结构所困扰。今天，我将带大家一起深入理解这个改变AI领域的技术。")]),t._v(" "),s("h2",{attrs:{id:"transformer架构概述"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#transformer架构概述"}},[t._v("#")]),t._v(" Transformer架构概述")]),t._v(" "),s("p",[t._v("Transformer是一种基于自注意力机制的神经网络架构，彻底摒弃了传统的RNN和LSTM结构。它的核心思想是："),s("strong",[t._v("序列中的每个元素都可以直接与其他所有元素交互，而不必依赖于中间元素")]),t._v("。")]),t._v(" "),s("p",[t._v("这种架构主要包含两个关键组件：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("编码器")]),t._v("：负责理解输入序列")]),t._v(" "),s("li",[s("strong",[t._v("解码器")]),t._v("：负责生成输出序列")])]),t._v(" "),s("p",[t._v("在标准的Transformer模型中，编码器和解码器都由多个相同的层堆叠而成。")]),t._v(" "),s("h2",{attrs:{id:"自注意力机制-核心创新"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#自注意力机制-核心创新"}},[t._v("#")]),t._v(" 自注意力机制：核心创新")]),t._v(" "),s("p",[t._v('自注意力机制是Transformer的精髓所在。它允许模型在处理序列中的某个元素时，能够"关注"序列中的其他所有元素，并计算它们的重要性权重。')]),t._v(" "),s("h3",{attrs:{id:"自注意力的数学表达"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#自注意力的数学表达"}},[t._v("#")]),t._v(" 自注意力的数学表达")]),t._v(" "),s("p",[t._v("自注意力的计算过程可以分为以下几步：")]),t._v(" "),s("ol",[s("li",[s("p",[s("strong",[t._v("线性变换")]),t._v("：将输入向量X通过三个不同的权重矩阵(Wq, Wk, Wv)分别转换为查询(Query)、键(Key)和值(Value)向量。")]),t._v(" "),s("div",{staticClass:"language- line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("Q = XWq\nK = XWk\nV = XWv\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br")])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("注意力分数计算")]),t._v("：通过查询和键向量计算注意力分数。")]),t._v(" "),s("div",{staticClass:"language- line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("Attention Scores = QK^T / √dk\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br")])]),s("p",[t._v("其中，dk是键向量的维度，除以√dk是为了防止点积过大导致softmax梯度消失。")])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("权重归一化")]),t._v("：使用softmax函数将注意力分数转换为概率分布。")]),t._v(" "),s("div",{staticClass:"language- line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("Weights = softmax(Attention Scores)\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br")])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("加权求和")]),t._v("：将权重与值向量相乘，得到最终的输出。")]),t._v(" "),s("div",{staticClass:"language- line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("Output = WeightsV\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br")])])])]),t._v(" "),s("h3",{attrs:{id:"多头注意力机制"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#多头注意力机制"}},[t._v("#")]),t._v(" 多头注意力机制")]),t._v(" "),s("p",[t._v('为了捕捉不同类型的关联信息，Transformer引入了多头注意力机制。它将自注意力计算分为多个"头"，每个头都有自己的参数矩阵，最后将所有头的输出连接起来并通过一个线性层。')]),t._v(" "),s("div",{staticClass:"language- line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("MultiHead(Q, K, V) = Concat(head1, head2, ..., headh)Wo\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br")])]),s("p",[t._v("其中，每个头的计算为：")]),t._v(" "),s("div",{staticClass:"language- line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("headi = Attention(QWiQ, KWiK, VWiV)\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br")])]),s("h2",{attrs:{id:"编码器结构"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#编码器结构"}},[t._v("#")]),t._v(" 编码器结构")]),t._v(" "),s("p",[t._v("Transformer的编码器由N个相同的层堆叠而成，每层包含两个子层：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("多头自注意力子层")]),t._v("：允许序列中的每个位置关注序列中的所有位置。")]),t._v(" "),s("li",[s("strong",[t._v("前馈神经网络子层")]),t._v("：由两个线性变换和一个ReLU激活函数组成。")])]),t._v(" "),s("p",[t._v("这两个子层都使用了残差连接和层归一化技术，以缓解梯度消失问题并加速训练。")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 伪代码表示编码器层的结构")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("encoder_layer")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 多头自注意力")]),t._v("\n    attn_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" multi_head_attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    attn_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" layer_norm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" attn_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 残差连接 + 层归一化")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 前馈网络")]),t._v("\n    ff_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" feed_forward_network"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("attn_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    ff_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" layer_norm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("attn_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" ff_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 残差连接 + 层归一化")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" ff_output\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br")])]),s("h2",{attrs:{id:"解码器结构"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#解码器结构"}},[t._v("#")]),t._v(" 解码器结构")]),t._v(" "),s("p",[t._v("解码器同样由N个相同的层堆叠而成，每层包含三个子层：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("掩码多头自注意力子层")]),t._v("：与编码器的自注意力类似，但会掩盖未来的位置，防止信息泄漏。")]),t._v(" "),s("li",[s("strong",[t._v("编码器-解码器注意力子层")]),t._v("：允许解码器关注编码器的输出。")]),t._v(" "),s("li",[s("strong",[t._v("前馈神经网络子层")]),t._v("：与编码器中的相同。")])]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 伪代码表示解码器层的结构")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("decoder_layer")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encoder_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 掩码多头自注意力")]),t._v("\n    masked_attn_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" masked_multi_head_attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    masked_attn_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" layer_norm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" masked_attn_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 编码器-解码器注意力")]),t._v("\n    enc_dec_attn_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" multi_head_attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("masked_attn_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encoder_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encoder_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    enc_dec_attn_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" layer_norm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("masked_attn_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" enc_dec_attn_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 前馈网络")]),t._v("\n    ff_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" feed_forward_network"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("enc_dec_attn_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    ff_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" layer_norm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("enc_dec_attn_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" ff_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" ff_output\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br"),s("span",{staticClass:"line-number"},[t._v("14")]),s("br"),s("span",{staticClass:"line-number"},[t._v("15")]),s("br")])]),s("h2",{attrs:{id:"位置编码-弥补顺序信息"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#位置编码-弥补顺序信息"}},[t._v("#")]),t._v(" 位置编码：弥补顺序信息")]),t._v(" "),s("p",[t._v("由于Transformer没有RNN那样的顺序处理机制，它需要一种方式来表示序列中元素的位置信息。这就是位置编码的作用。")]),t._v(" "),s("p",[t._v("位置编码使用正弦和余弦函数来生成位置向量：")]),t._v(" "),s("div",{staticClass:"language- line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br")])]),s("p",[t._v("其中，pos是位置索引，i是维度索引，d_model是模型的维度。")]),t._v(" "),s("h2",{attrs:{id:"为什么transformer如此强大"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#为什么transformer如此强大"}},[t._v("#")]),t._v(" 为什么Transformer如此强大？")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("并行计算能力")]),t._v("：与RNN不同，Transformer可以并行处理整个序列，大大提高了训练效率。")]),t._v(" "),s("li",[s("strong",[t._v("长距离依赖捕捉")]),t._v("：自注意力机制可以直接连接序列中任意两个位置，不受距离限制。")]),t._v(" "),s("li",[s("strong",[t._v("上下文理解能力")]),t._v("：通过注意力权重，模型能够显式地表示不同元素之间的关系。")]),t._v(" "),s("li",[s("strong",[t._v("可扩展性")]),t._v("：Transformer架构可以轻松扩展到非常大的模型规模。")])]),t._v(" "),s("h2",{attrs:{id:"结语"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#结语"}},[t._v("#")]),t._v(" 结语")]),t._v(" "),s("p",[t._v("Transformer架构和自注意力机制是现代大型语言模型的基石，理解它们对于深入掌握LLM技术至关重要。从GPT系列到BERT，再到PaLM和LLaMA，几乎所有知名的大语言模型都基于这一架构。随着技术的不断发展，我们可能会看到更多基于Transformer的创新变体，但这一核心思想仍将继续引领AI领域的发展。")]),t._v(" "),s("blockquote",[s("p",[t._v('"The attention mechanism is a concept that allows models to focus on specific parts of the input data when producing an output. It\'s like having a spotlight that can move around to illuminate different parts of the stage as needed." — 来自《Attention Is All You Need》论文的启发')])]),t._v(" "),s("p",[t._v("希望这篇文章能帮助你更好地理解Transformer架构和自注意力机制。如果你有任何问题或想法，欢迎在评论区交流讨论！")])])}),[],!1,null,null,null);s.default=e.exports}}]);