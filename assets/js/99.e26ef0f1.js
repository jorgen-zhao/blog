(window.webpackJsonp=window.webpackJsonp||[]).push([[99],{451:function(t,s,a){"use strict";a.r(s);var n=a(15),r=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"前言"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#前言"}},[t._v("#")]),t._v(" 前言")]),t._v(" "),s("p",[t._v("随着大语言模型(LLM)规模的不断扩大，完整的模型微调变得越来越资源密集和昂贵。一个拥有数十亿甚至数千亿参数的模型，如果进行全参数微调，需要巨大的计算资源和存储空间。这使得许多研究者和开发者难以针对特定任务或领域定制这些强大的模型。")]),t._v(" "),s("p",[t._v("幸运的是，近年来一系列"),s("strong",[t._v("参数高效微调")]),t._v("(Parameter-Efficient Fine-Tuning, PEFT)技术应运而生，它们通过只微调模型参数的一小部分，就能实现与全参数微调相媲美的效果。本文将深入探讨这些创新技术，它们如何工作，以及它们在实际应用中的价值和局限性。")]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("提示")]),t._v(" "),s("p",[t._v('"在AI领域，效率与性能的平衡往往是创新的关键。参数高效微调技术正是这种平衡的完美体现，它让我们能够在有限资源下释放无限可能。"')])]),t._v(" "),s("h2",{attrs:{id:"参数高效微调的兴起"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参数高效微调的兴起"}},[t._v("#")]),t._v(" 参数高效微调的兴起")]),t._v(" "),s("p",[t._v("传统的全参数微调需要更新模型的所有参数，对于一个拥有1750亿参数的模型(如GPT-3)，这意味着需要更新1750亿个参数。这不仅需要大量的GPU内存，还需要大量的计算时间。")]),t._v(" "),s("p",[t._v("相比之下，参数高效微调技术通常只更新模型参数的0.1%到1%，却能达到与全参数微调相当甚至更好的性能。这种显著的效率提升使得在消费级硬件上微调大型模型成为可能。")]),t._v(" "),s("h2",{attrs:{id:"主流参数高效微调方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#主流参数高效微调方法"}},[t._v("#")]),t._v(" 主流参数高效微调方法")]),t._v(" "),s("h3",{attrs:{id:"_1-lora-low-rank-adaptation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-lora-low-rank-adaptation"}},[t._v("#")]),t._v(" 1. LoRA (Low-Rank Adaptation)")]),t._v(" "),s("p",[s("strong",[t._v("LoRA")]),t._v("是目前最受欢迎的参数高效微调方法之一，由微软研究院的研究人员提出。")]),t._v(" "),s("h4",{attrs:{id:"工作原理"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#工作原理"}},[t._v("#")]),t._v(" 工作原理")]),t._v(" "),s("p",[t._v("LoRA的核心思想是，在预训练权重矩阵$W_0$上添加一个低秩矩阵$\\Delta W$：")]),t._v(" "),s("p",[t._v("$$W = W_0 + \\Delta W$$")]),t._v(" "),s("p",[t._v("其中$\\Delta W$被分解为两个较小的矩阵$A$和$B$的乘积：\n$$\\Delta W = BA$$")]),t._v(" "),s("p",[t._v("矩阵$A$的维度是$r \\times d$，矩阵$B$的维度是$d \\times r$，其中$r$远小于$d$。这样，虽然$\\Delta W$的维度与$W_0$相同($d \\times d$)，但可训练的参数数量从$d^2$减少到了$2 \\times r \\times d$。")]),t._v(" "),s("p",[t._v("在实际应用中，我们冻结原始权重$W_0$，只训练低秩矩阵$A$和$B$。前向传播计算为：\n$$x \\rightarrow W_0x + BAx$$")]),t._v(" "),s("h4",{attrs:{id:"优势"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#优势"}},[t._v("#")]),t._v(" 优势")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("参数效率")]),t._v("：可训练参数数量大幅减少，通常只有全参数微调的0.1%到1%。")]),t._v(" "),s("li",[s("strong",[t._v("存储效率")]),t._v("：微调后的模型可以仅存储低秩矩阵，大大减少了存储需求。")]),t._v(" "),s("li",[s("strong",[t._v("灵活性")]),t._v("：可以轻松地在不同任务间切换，只需加载不同的低秩矩阵。")]),t._v(" "),s("li",[s("strong",[t._v("性能")]),t._v("：在多个基准测试中，LoRA的表现与全参数微调相当，甚至在某些任务上更优。")])]),t._v(" "),s("h3",{attrs:{id:"_2-prefix-tuning"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-prefix-tuning"}},[t._v("#")]),t._v(" 2. Prefix Tuning")]),t._v(" "),s("p",[s("strong",[t._v("Prefix Tuning")]),t._v('是由斯坦福大学和谷歌研究人员提出的方法，它不修改模型的权重，而是在输入前添加特定"前缀"。')]),t._v(" "),s("h4",{attrs:{id:"工作原理-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#工作原理-2"}},[t._v("#")]),t._v(" 工作原理")]),t._v(" "),s("p",[t._v("Prefix Tuning在输入序列的开头添加可学习的连续向量(称为前缀)，这些向量作为软提示(soft prompts)引导模型生成特定任务的输出。对于自回归模型(如GPT)，前缀被添加到输入序列的开头；对于编码器-解码器模型(如T5)，前缀被添加到输入和输出序列的开头。")]),t._v(" "),s("p",[t._v('这些前缀参数是随机初始化的，并在微调过程中更新。由于前缀是连续向量而非离散标记，因此被称为"软提示"。')]),t._v(" "),s("h4",{attrs:{id:"实现细节"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#实现细节"}},[t._v("#")]),t._v(" 实现细节")]),t._v(" "),s("p",[t._v("对于Transformer模型，前缀被注入到注意力层的键和值投影中。具体来说，对于第$i$层的前缀$P_i$，其维度为$k \\times d$，其中$k$是前缀长度，$d$是隐藏维度。")]),t._v(" "),s("p",[t._v("在计算注意力时，前缀$P_i$被添加到键和值矩阵中：\n$$K_i = [P_i; XW_K]$$\n$$V_i = [P_i; XW_V]$$")]),t._v(" "),s("p",[t._v("其中$X$是输入序列，$W_K$和$W_V$是键和值的投影矩阵。")]),t._v(" "),s("h4",{attrs:{id:"优势-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#优势-2"}},[t._v("#")]),t._v(" 优势")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("无需修改原始模型")]),t._v("：前缀参数与原始模型分离，可以独立存储和加载。")]),t._v(" "),s("li",[s("strong",[t._v("任务适应性")]),t._v("：不同的任务可以有不同的前缀，实现多任务学习。")]),t._v(" "),s("li",[s("strong",[t._v("性能")]),t._v("：在多个NLP任务上表现优异，特别是在少样本学习场景中。")])]),t._v(" "),s("h3",{attrs:{id:"_3-adapter-tuning"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-adapter-tuning"}},[t._v("#")]),t._v(" 3. Adapter Tuning")]),t._v(" "),s("p",[s("strong",[t._v("Adapter Tuning")]),t._v("是一种在Transformer模型的每一层之间插入小型适配器模块的方法。")]),t._v(" "),s("h4",{attrs:{id:"工作原理-3"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#工作原理-3"}},[t._v("#")]),t._v(" 工作原理")]),t._v(" "),s("p",[t._v("Adapter Tuning在Transformer的每一层之间插入两个小型前馈网络：")]),t._v(" "),s("ol",[s("li",[t._v("一个下投影层，将维度从$d$降到$r$（$r \\ll d$）")]),t._v(" "),s("li",[t._v("一个上投影层，将维度从$r$升回$d$")])]),t._v(" "),s("p",[t._v("在训练过程中，原始Transformer层的权重被冻结，只有适配器模块的参数被更新。前向传播计算为：\n$$x \\rightarrow \\text{LayerNorm}(x) \\rightarrow \\text{FFN}(\\text{DownProj}(\\text{Activation}(\\text{UpProj}(\\text{DownProj}(x))))) + x$$")]),t._v(" "),s("h4",{attrs:{id:"变体"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#变体"}},[t._v("#")]),t._v(" 变体")]),t._v(" "),s("p",[t._v("Adapter Tuning有多种变体：")]),t._v(" "),s("ul",[s("li",[t._v("** bottleneck adapters**：使用瓶颈结构，先降维再升维")]),t._v(" "),s("li",[t._v("** parallel adapters**：适配器与原始路径并行计算")]),t._v(" "),s("li",[t._v("** mixed adapters**：不同层使用不同类型的适配器")])]),t._v(" "),s("h4",{attrs:{id:"优势-3"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#优势-3"}},[t._v("#")]),t._v(" 优势")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("模块化")]),t._v("：适配器可以即插即用，方便组合不同任务的适配器")]),t._v(" "),s("li",[s("strong",[t._v("可解释性")]),t._v("：适配器的作用相对独立，更容易分析")]),t._v(" "),s("li",[s("strong",[t._v("性能")]),t._v("：在多种NLP任务上表现优异，特别是在迁移学习场景中")])]),t._v(" "),s("h3",{attrs:{id:"_4-其他参数高效微调方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-其他参数高效微调方法"}},[t._v("#")]),t._v(" 4. 其他参数高效微调方法")]),t._v(" "),s("p",[t._v("除了上述方法，还有许多其他创新的PEFT技术：")]),t._v(" "),s("h4",{attrs:{id:"p-tuning-v2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#p-tuning-v2"}},[t._v("#")]),t._v(" P-Tuning v2")]),t._v(" "),s("p",[t._v("P-Tuning v2是P-Tuning的改进版本，它将可学习的提示嵌入到Transformer的每一层，而不仅仅是输入层。这种方法在多个NLP任务上取得了优异的性能。")]),t._v(" "),s("h4",{attrs:{id:"ia3-insertion-based-adaptive-adapters"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#ia3-insertion-based-adaptive-adapters"}},[t._v("#")]),t._v(" IA³ (Insertion-based Adaptive Adapters)")]),t._v(" "),s("p",[t._v("IA³是一种创新的适配器方法，它不是在固定位置插入适配器，而是动态地在Transformer层之间插入适配器，根据输入内容自适应地选择插入位置。")]),t._v(" "),s("h4",{attrs:{id:"dora-weight-decomposed-low-rank-adaptation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#dora-weight-decomposed-low-rank-adaptation"}},[t._v("#")]),t._v(" DoRA (Weight-Decomposed Low-Rank Adaptation)")]),t._v(" "),s("p",[t._v("DoRA是LoRA的改进版本，它将权重矩阵分解为幅度矩阵和方向矩阵，并分别优化这两个矩阵。这种方法在某些任务上表现优于标准LoRA。")]),t._v(" "),s("h2",{attrs:{id:"参数高效微调的实践应用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参数高效微调的实践应用"}},[t._v("#")]),t._v(" 参数高效微调的实践应用")]),t._v(" "),s("h3",{attrs:{id:"hugging-face-peft库"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#hugging-face-peft库"}},[t._v("#")]),t._v(" Hugging Face PEFT库")]),t._v(" "),s("p",[t._v("Hugging Face提供了强大的PEFT库，支持多种参数高效微调方法：")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" peft "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LoraConfig"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" get_peft_model\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 配置LoRA")]),t._v("\nlora_config "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LoraConfig"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    r"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 低秩矩阵的秩")]),t._v("\n    lora_alpha"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 缩放因子")]),t._v("\n    target_modules"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"q_proj"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"v_proj"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 应用LoRA的层")]),t._v("\n    lora_dropout"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.05")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    bias"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"none"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 应用LoRA到模型")]),t._v("\nmodel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_peft_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lora_config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br")])]),s("h3",{attrs:{id:"微调示例"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#微调示例"}},[t._v("#")]),t._v(" 微调示例")]),t._v(" "),s("p",[t._v("以下是一个使用LoRA微调BERT模型进行文本分类的简单示例：")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" BertTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" BertForSequenceClassification"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Trainer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TrainingArguments\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" peft "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LoraConfig"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" get_peft_model\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 加载预训练模型和分词器")]),t._v("\nmodel_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bert-base-uncased"')]),t._v("\ntokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BertTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BertForSequenceClassification"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 配置LoRA")]),t._v("\nlora_config "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LoraConfig"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    r"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    lora_alpha"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    target_modules"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"query"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"value"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    lora_dropout"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    bias"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"none"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 应用LoRA")]),t._v("\nmodel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_peft_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lora_config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 准备数据集")]),t._v("\ntrain_texts "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I love this product!"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This is terrible."')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ntrain_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ntrain_encodings "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_texts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" truncation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("CustomDataset")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encodings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encodings "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" encodings\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" labels\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__getitem__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        item "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("val"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encodings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" item\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__len__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrain_dataset "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" CustomDataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_encodings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 设置训练参数")]),t._v("\ntraining_args "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainingArguments"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output_dir"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./results"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_train_epochs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    per_device_train_batch_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    save_steps"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10_000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    save_total_limit"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建训练器并开始训练")]),t._v("\ntrainer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    model"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    args"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("training_args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrainer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br"),s("span",{staticClass:"line-number"},[t._v("14")]),s("br"),s("span",{staticClass:"line-number"},[t._v("15")]),s("br"),s("span",{staticClass:"line-number"},[t._v("16")]),s("br"),s("span",{staticClass:"line-number"},[t._v("17")]),s("br"),s("span",{staticClass:"line-number"},[t._v("18")]),s("br"),s("span",{staticClass:"line-number"},[t._v("19")]),s("br"),s("span",{staticClass:"line-number"},[t._v("20")]),s("br"),s("span",{staticClass:"line-number"},[t._v("21")]),s("br"),s("span",{staticClass:"line-number"},[t._v("22")]),s("br"),s("span",{staticClass:"line-number"},[t._v("23")]),s("br"),s("span",{staticClass:"line-number"},[t._v("24")]),s("br"),s("span",{staticClass:"line-number"},[t._v("25")]),s("br"),s("span",{staticClass:"line-number"},[t._v("26")]),s("br"),s("span",{staticClass:"line-number"},[t._v("27")]),s("br"),s("span",{staticClass:"line-number"},[t._v("28")]),s("br"),s("span",{staticClass:"line-number"},[t._v("29")]),s("br"),s("span",{staticClass:"line-number"},[t._v("30")]),s("br"),s("span",{staticClass:"line-number"},[t._v("31")]),s("br"),s("span",{staticClass:"line-number"},[t._v("32")]),s("br"),s("span",{staticClass:"line-number"},[t._v("33")]),s("br"),s("span",{staticClass:"line-number"},[t._v("34")]),s("br"),s("span",{staticClass:"line-number"},[t._v("35")]),s("br"),s("span",{staticClass:"line-number"},[t._v("36")]),s("br"),s("span",{staticClass:"line-number"},[t._v("37")]),s("br"),s("span",{staticClass:"line-number"},[t._v("38")]),s("br"),s("span",{staticClass:"line-number"},[t._v("39")]),s("br"),s("span",{staticClass:"line-number"},[t._v("40")]),s("br"),s("span",{staticClass:"line-number"},[t._v("41")]),s("br"),s("span",{staticClass:"line-number"},[t._v("42")]),s("br"),s("span",{staticClass:"line-number"},[t._v("43")]),s("br"),s("span",{staticClass:"line-number"},[t._v("44")]),s("br"),s("span",{staticClass:"line-number"},[t._v("45")]),s("br"),s("span",{staticClass:"line-number"},[t._v("46")]),s("br"),s("span",{staticClass:"line-number"},[t._v("47")]),s("br"),s("span",{staticClass:"line-number"},[t._v("48")]),s("br"),s("span",{staticClass:"line-number"},[t._v("49")]),s("br"),s("span",{staticClass:"line-number"},[t._v("50")]),s("br"),s("span",{staticClass:"line-number"},[t._v("51")]),s("br"),s("span",{staticClass:"line-number"},[t._v("52")]),s("br"),s("span",{staticClass:"line-number"},[t._v("53")]),s("br"),s("span",{staticClass:"line-number"},[t._v("54")]),s("br"),s("span",{staticClass:"line-number"},[t._v("55")]),s("br"),s("span",{staticClass:"line-number"},[t._v("56")]),s("br"),s("span",{staticClass:"line-number"},[t._v("57")]),s("br")])]),s("h3",{attrs:{id:"性能与资源对比"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#性能与资源对比"}},[t._v("#")]),t._v(" 性能与资源对比")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("方法")]),t._v(" "),s("th",[t._v("可训练参数比例")]),t._v(" "),s("th",[t._v("内存需求")]),t._v(" "),s("th",[t._v("训练时间")]),t._v(" "),s("th",[t._v("推理速度")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("全参数微调")]),t._v(" "),s("td",[t._v("100%")]),t._v(" "),s("td",[t._v("高")]),t._v(" "),s("td",[t._v("长")]),t._v(" "),s("td",[t._v("慢")])]),t._v(" "),s("tr",[s("td",[t._v("LoRA")]),t._v(" "),s("td",[t._v("0.1%-1%")]),t._v(" "),s("td",[t._v("低")]),t._v(" "),s("td",[t._v("短")]),t._v(" "),s("td",[t._v("快")])]),t._v(" "),s("tr",[s("td",[t._v("Prefix Tuning")]),t._v(" "),s("td",[t._v("0.1%-0.6%")]),t._v(" "),s("td",[t._v("中")]),t._v(" "),s("td",[t._v("中")]),t._v(" "),s("td",[t._v("中")])]),t._v(" "),s("tr",[s("td",[t._v("Adapter Tuning")]),t._v(" "),s("td",[t._v("0.5%-2%")]),t._v(" "),s("td",[t._v("中")]),t._v(" "),s("td",[t._v("中")]),t._v(" "),s("td",[t._v("中")])])])]),t._v(" "),s("p",[t._v("从上表可以看出，参数高效微调方法在资源消耗方面具有显著优势，同时保持了与全参数微调相当的性能。")]),t._v(" "),s("h2",{attrs:{id:"参数高效微调的挑战与局限"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参数高效微调的挑战与局限"}},[t._v("#")]),t._v(" 参数高效微调的挑战与局限")]),t._v(" "),s("p",[t._v("尽管参数高效微调技术带来了诸多好处，但它们也面临一些挑战和局限：")]),t._v(" "),s("h3",{attrs:{id:"_1-任务特定性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-任务特定性"}},[t._v("#")]),t._v(" 1. 任务特定性")]),t._v(" "),s("p",[t._v("大多数参数高效微调方法是为特定任务设计的，在不同任务间迁移可能需要重新调整方法。例如，为分类任务微调的LoRA模型可能不适用于文本生成任务。")]),t._v(" "),s("h3",{attrs:{id:"_2-架构依赖性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-架构依赖性"}},[t._v("#")]),t._v(" 2. 架构依赖性")]),t._v(" "),s("p",[t._v("不同的参数高效微调方法对模型架构有不同的依赖性。例如，Prefix Tuning在自回归模型和编码器-解码器模型中的实现方式不同。")]),t._v(" "),s("h3",{attrs:{id:"_3-性能权衡"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-性能权衡"}},[t._v("#")]),t._v(" 3. 性能权衡")]),t._v(" "),s("p",[t._v("在某些复杂任务上，参数高效微调可能无法达到全参数微调的性能。特别是在需要大幅改变模型行为的任务中，微调更多参数可能更有效。")]),t._v(" "),s("h3",{attrs:{id:"_4-超参数敏感性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-超参数敏感性"}},[t._v("#")]),t._v(" 4. 超参数敏感性")]),t._v(" "),s("p",[t._v("参数高效微调方法通常对超参数选择较为敏感，如LoRA的秩$r$、学习率等。需要仔细调整这些参数以获得最佳性能。")]),t._v(" "),s("h2",{attrs:{id:"未来展望"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#未来展望"}},[t._v("#")]),t._v(" 未来展望")]),t._v(" "),s("p",[t._v("参数高效微调领域仍在快速发展，未来可能出现以下趋势：")]),t._v(" "),s("h3",{attrs:{id:"_1-自适应微调方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-自适应微调方法"}},[t._v("#")]),t._v(" 1. 自适应微调方法")]),t._v(" "),s("p",[t._v("未来的方法可能会更加智能化，能够根据任务复杂性和资源限制自动选择最合适的微调策略。")]),t._v(" "),s("h3",{attrs:{id:"_2-跨任务迁移能力"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-跨任务迁移能力"}},[t._v("#")]),t._v(" 2. 跨任务迁移能力")]),t._v(" "),s("p",[t._v("提高参数高效微调方法在不同任务间的迁移能力，使得一个微调后的模型可以更好地适应多种任务。")]),t._v(" "),s("h3",{attrs:{id:"_3-多模态扩展"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-多模态扩展"}},[t._v("#")]),t._v(" 3. 多模态扩展")]),t._v(" "),s("p",[t._v("将参数高效微调技术扩展到多模态模型，使得视觉、语言和音频模型能够高效地适应特定领域。")]),t._v(" "),s("h3",{attrs:{id:"_4-硬件协同设计"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-硬件协同设计"}},[t._v("#")]),t._v(" 4. 硬件协同设计")]),t._v(" "),s("p",[t._v("与硬件设计相结合，开发专门针对参数高效微调优化的芯片和加速器，进一步提高效率。")]),t._v(" "),s("h2",{attrs:{id:"结语"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#结语"}},[t._v("#")]),t._v(" 结语")]),t._v(" "),s("p",[t._v("参数高效微调技术为大型语言模型的定制化提供了一条经济高效的路径。通过LoRA、Prefix Tuning、Adapter Tuning等方法，我们能够在资源受限的环境下，充分发挥大语言模型的潜力。")]),t._v(" "),s("p",[t._v("随着这些技术的不断发展和完善，我们可以预见，参数高效微调将成为大模型应用的标准配置，使得更多研究者和开发者能够参与到AI创新中来。无论是在学术研究还是工业应用中，这些技术都将发挥越来越重要的作用。")]),t._v(" "),s("blockquote",[s("p",[t._v('"在AI领域，效率与性能的平衡往往是创新的关键。参数高效微调技术正是这种平衡的完美体现，它让我们能够在有限资源下释放无限可能。"')])])])}),[],!1,null,null,null);s.default=r.exports}}]);