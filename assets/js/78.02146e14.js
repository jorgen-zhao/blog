(window.webpackJsonp=window.webpackJsonp||[]).push([[78],{424:function(a,t,e){"use strict";e.r(t);var r=e(14),v=Object(r.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("p",[a._v("“大数据”从字面上可以翻译为“大量、海量的数据”，表示无法在一定时间范围内使用常规软件工具进行采集、存储和处理的数据集合。\n")]),a._v(" "),t("p",[t("img",{attrs:{src:"/tool/GE/300/big-data.jpg",alt:"大数据处理流程图"}})]),a._v(" "),t("p",[a._v("上图是一个简化的大数据处理流程图，大数据处理的主要流程包括数据收集、数据存储、数据处理、数据应用等主要环节。")]),a._v(" "),t("h2",{attrs:{id:"数据收集"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据收集"}},[a._v("#")]),a._v(" 数据收集")]),a._v(" "),t("p",[a._v("大数据处理的第一步是数据的收集。现在的中大型项目通常采用微服务架构进行分布式部署，所以数据的采集需要在多台服务器上进行，且采集过程不能影响正常业务的开展。基于这种需求，就衍生了多种日志收集工具，如 Flume 、Logstash、Kibana 等，它们都能通过简单的配置完成复杂的数据收集和数据聚合。")]),a._v(" "),t("h2",{attrs:{id:"数据存储"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据存储"}},[a._v("#")]),a._v(" 数据存储")]),a._v(" "),t("p",[a._v("收集到数据后，下一个问题就是：数据该如何进行存储？通常大家最为熟知是 MySQL、Oracle 等传统的关系型数据库，它们的优点是能够快速存储结构化的数据，并支持随机访问。但大数据的数据结构通常是半结构化（如日志数据）、甚至是非结构化的（如视频、音频数据），为了解决海量半结构化和非结构化数据的存储，衍生了 Hadoop HDFS 、KFS、GFS 等分布式文件系统，它们都能够支持结构化、半结构和非结构化数据的存储，并可以通过增加机器进行横向扩展。")]),a._v(" "),t("p",[a._v("分布式文件系统完美地解决了海量数据存储的问题，但是一个优秀的数据存储系统需要同时考虑数据存储和访问两方面的问题，比如你希望能够对数据进行随机访问，这是传统的关系型数据库所擅长的，但却不是分布式文件系统所擅长的，那么有没有一种存储方案能够同时兼具分布式文件系统和关系型数据库的优点，基于这种需求，就产生了 HBase、MongoDB。")]),a._v(" "),t("h2",{attrs:{id:"数据分析"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据分析"}},[a._v("#")]),a._v(" 数据分析")]),a._v(" "),t("p",[a._v("大数据处理最重要的环节就是数据分析，数据分析通常分为两种：批处理和流处理。")]),a._v(" "),t("ul",[t("li",[a._v("批处理：对一段时间内海量的离线数据进行统一的处理，对应的处理框架有 Hadoop MapReduce、Spark、Flink 等；")]),a._v(" "),t("li",[a._v("流处理：对运动中的数据进行处理，即在接收数据的同时就对其进行处理，对应的处理框架有 Storm、Spark Streaming、Flink Streaming 等。")])]),a._v(" "),t("p",[a._v("批处理和流处理各有其适用的场景，时间不敏感或者硬件资源有限，可以采用批处理；时间敏感和及时性要求高就可以采用流处理。随着服务器硬件的价格越来越低和大家对及时性的要求越来越高，流处理越来越普遍，如股票价格预测和电商运营数据分析等。")]),a._v(" "),t("p",[a._v("上面的框架都是需要通过编程来进行数据分析，那么如果你不是一个后台工程师，是不是就不能进行数据的分析了？当然不是，大数据是一个非常完善的生态圈，有需求就有解决方案。")]),a._v(" "),t("p",[a._v("为了能够让熟悉 SQL 的人员也能够进行数据的分析，查询分析框架应运而生，常用的有 Hive 、Spark SQL 、Flink SQL、 Pig、Phoenix 等。这些框架都能够使用标准的 SQL 或者 类 SQL 语法灵活地进行数据的查询分析。这些 SQL 经过解析优化后转换为对应的作业程序来运行，如 Hive 本质上就是将 SQL 转换为 MapReduce 作业，Spark SQL 将 SQL 转换为一系列的 RDDs 和转换关系（transformations），Phoenix 将 SQL 查询转换为一个或多个 HBase Scan。")]),a._v(" "),t("h2",{attrs:{id:"数据应用"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据应用"}},[a._v("#")]),a._v(" 数据应用")]),a._v(" "),t("p",[a._v("数据分析完成后，接下来就是数据应用的范畴，这取决于你实际的业务需求。比如你可以将数据进行可视化展现，或者将数据用于优化你的推荐算法，这种运用现在很普遍，比如短视频个性化推荐、电商商品推荐、头条新闻推荐等。当然你也可以将数据用于训练你的机器学习模型，这些都属于其他领域的范畴，都有着对应的框架和技术栈进行处理，这里就不一一赘述。")]),a._v(" "),t("h2",{attrs:{id:"其他框架"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#其他框架"}},[a._v("#")]),a._v(" 其他框架")]),a._v(" "),t("p",[a._v("上面是一个标准的大数据处理流程所用到的技术框架。但是实际的大数据处理流程比上面复杂很多，针对大数据处理中的各种复杂问题分别衍生了各类框架：")]),a._v(" "),t("p",[a._v("单机的处理能力都是存在瓶颈的，所以大数据框架都是采用集群模式进行部署，为了更方便的进行集群的部署、监控和管理，衍生了 Ambari、Cloudera Manager 等集群管理工具；")]),a._v(" "),t("p",[a._v("想要保证集群高可用，需要用到 ZooKeeper ，ZooKeeper 是最常用的分布式协调服务，它能够解决大多数集群问题，包括首领选举、失败恢复、元数据存储及其一致性保证。同时针对集群资源管理的需求，又衍生了 Hadoop YARN ;\n复杂大数据处理的另外一个显著的问题是，如何调度多个复杂的并且彼此之间存在依赖关系的作业？基于这种需求，产生了 Azkaban 和 Oozie 等工作流调度框架；")]),a._v(" "),t("p",[a._v("大数据流处理中使用的比较多的另外一个框架是 Kafka，它可以用于消峰，避免在秒杀等场景下并发数据对流处理程序造成冲击；\n另一个常用的框架是 Sqoop ，主要是解决了数据迁移的问题，它能够通过简单的命令将关系型数据库中的数据导入到 HDFS 、Hive 或 HBase 中，或者从 HDFS 、Hive 导出到关系型数据库上。")])])}),[],!1,null,null,null);t.default=v.exports}}]);